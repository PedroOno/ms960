{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imageMNIST.csv\", header=None, low_memory=False)\n",
    "df_y = pd.read_csv(\"labelMNIST.csv\", header=None)\n",
    "\n",
    "def change_type(x):\n",
    "    if(isinstance(x, int)):\n",
    "        return float(x)\n",
    "    elif(isinstance(x, str)):\n",
    "        return float(x.replace(\",\", \".\"))\n",
    "\n",
    "df = df.applymap(change_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoide\n",
    "def sigmoid(z):\n",
    "    g = (1 + np.exp(-z))**(-1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid*(1-sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(X,y,theta,size_in,size_hidden,size_out, Lambda):\n",
    "    epsolon = 1e-4\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "#     theta_p1 = theta1+epsolon \n",
    "#     theta_m1 = theta1-epsolon\n",
    "    theta_p1 = np.array([theta1+epsolon, theta2], dtype=object)\n",
    "    theta_m1 = np.array([theta1-epsolon, theta2], dtype=object)\n",
    "    \n",
    "    theta_p2 = np.array([theta1, theta2+epsolon], dtype=object)\n",
    "    theta_m2 = np.array([theta1, theta2-epsolon], dtype=object)\n",
    "    \n",
    "    J_p1, grad1, grad2 = costFunction(X,y,theta_p1,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_p1, grad1_reg, grad2_reg = costFunction(X,y,theta_p1,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_m1, grad1, grad2 = costFunction(X,y,theta_m1,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_m1, grad1_reg, grad2_reg = costFunction(X,y,theta_m1,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_p2, grad1, grad2 = costFunction(X,y,theta_p2,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_p2, grad1_reg, grad2_reg = costFunction(X,y,theta_p2,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_m2, grad1, grad2 = costFunction(X,y,theta_m2,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_m2, grad1_reg, grad2_reg = costFunction(X,y,theta_m2,size_in,size_hidden,size_out, Lambda=True)\n",
    "    \n",
    "    J, grad1, grad2 = costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J, grad1_reg, grad2_reg = costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    check_delta1 = (J_p1-J_m1) / (2*epsolon)\n",
    "    check_delta2 = (J_p2-J_m2) / (2*epsolon)\n",
    "    \n",
    "    check_delta1_reg = (reg_J_p1-reg_J_m1) / (2*epsolon)\n",
    "    check_delta2_reg = (reg_J_p2-reg_J_m2) / (2*epsolon)\n",
    "    \n",
    "    diff1 = np.abs(check_delta1 - grad1)\n",
    "    diff2 = np.abs(check_delta2 - grad2)\n",
    "    diff1_reg = np.abs(check_delta1_reg - grad1_reg)\n",
    "    diff2_reg = np.abs(check_delta2_reg - grad2_reg)\n",
    "    \n",
    "    \n",
    "    return (diff1, diff2, diff1_reg, diff2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y_onehot = np.zeros((m,size_out))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    \n",
    "    y_onehot = np.zeros((y.size, 11))\n",
    "    y_onehot[np.arange(y.size),y.flatten()] = 1\n",
    "    y_onehot = y_onehot[:,1:]\n",
    "    \n",
    "    for j in range(size_out):\n",
    "        J = J + sum(-y_onehot[:,j]*np.log(a2[:,j])-(1-y_onehot[:,j])*np.log(1-a2[:,j]))\n",
    "        \n",
    "    J = 1/m*J\n",
    "    \n",
    "    if(reg):\n",
    "        J = J + Lambda/l(2*m)*(np.sum(theta1[:,1:]**2)+np.sum(theta2[:,1:]**2))\n",
    "                                 \n",
    "    grad1 = np.zeros((theta1.shape))\n",
    "    grad2 = np.zeros((theta2.shape))\n",
    "                                 \n",
    "    for i in range(m):\n",
    "        xi = X[i,:]\n",
    "        a1i = a1[i,:]\n",
    "        a2i = a2[i,:]\n",
    "        d2 = a2i - y_onehot[i,:]\n",
    "        d1 = theta2.T.dot(d2.T) * sigmoidGradient(np.hstack((1,xi.dot(theta1.T))))\n",
    "        grad1 = grad1 + d1[1:][:,np.newaxis].dot(xi[:,np.newaxis].T)\n",
    "        grad2 = grad2 + d2.T[:,np.newaxis].dot(a1i[:,np.newaxis].T)\n",
    "                                 \n",
    "    grad1 = grad1/m\n",
    "    grad2 = grad2/m\n",
    "    \n",
    "    if(reg):\n",
    "        grad1 = grad1 + (Lambda/m)*np.hstack((np.zeros((theta1.shape[0],1)),theta1[:,1:]))\n",
    "        grad2 = grad2 + (Lambda/m)*np.hstack((np.zeros((theta2.shape[0],1)),theta2[:,1:]))\n",
    "                                 \n",
    "    return J,grad1,grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##peguei do site amigo https://prateekvishnu.medium.com/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
    "def randInitializeXavier(size_in, size_out):\n",
    "    w=np.random.randn(size_out,size_in)*np.sqrt(1/size_in) \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(nbr_iter)):\n",
    "        theta = np.array([theta1, theta2], dtype=object)\n",
    "        if(reg):\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=True)\n",
    "        else:\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=False)\n",
    "        \n",
    "        theta1 = theta1 - (alpha*grad1)\n",
    "        theta2 = theta2 - (alpha*grad2)\n",
    "\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    nn_paramsFinal = np.array([theta1,theta2], dtype=object)\n",
    "    return nn_paramsFinal, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,theta1,theta2):\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    return np.argmax(a2,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train examples:\t3000\n",
      "# Validation examples:\t1000\n",
      "# Train examples:\t1000\n"
     ]
    }
   ],
   "source": [
    "X = df.values\n",
    "y = df_y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"# Train examples:\\t{}\".format(X_train.shape[0]))\n",
    "print(\"# Validation examples:\\t{}\".format(X_val.shape[0]))\n",
    "print(\"# Train examples:\\t{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532fc6f7b4d49588f84d806c223f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 50\n",
    "num_labels = 10\n",
    "\n",
    "initial_theta1 = randInitializeXavier(input_layer_size+1,hidden_layer_size+1)\n",
    "initial_theta2 = randInitializeXavier(hidden_layer_size+2,num_labels)\n",
    "initial_theta = np.array([initial_theta1, initial_theta2], dtype=object)\n",
    "\n",
    "# X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels):\n",
    "#0.001, 0.003, 0.01, 0.03, 0.1,\n",
    "alpha = 1\n",
    "theta,J_history_10 = gradientDescent(X_train,y_train,initial_theta,alpha,200,1,input_layer_size,hidden_layer_size,num_labels, reg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "theta1 = theta[0]\n",
    "theta2 = theta[1]\n",
    "pred = prediction(X,theta1,theta2)\n",
    "print(\"Training Set Accuracy:\", np.sum(pred==y.flatten())/len(y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(J_history_10, label=\"lambda={}\".format(alpha))\n",
    "plt.legend()\n",
    "# plt.savefig(\"lambda{}.jpg\".format(str(alpha)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232af3a3d8454313a7bfb07d63458b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311ae95b9d8d4a9f811409e164ba8030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small network\n",
    "samples = 50\n",
    "rand_index = random.sample([i for i in range(df.shape[0])], k=samples)\n",
    "\n",
    "X = df.iloc[rand_index].values\n",
    "y = df_y.iloc[rand_index].values\n",
    "\n",
    "\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 5\n",
    "num_labels = 10\n",
    "\n",
    "initial_theta1 = randInitializeXavier(input_layer_size+1,hidden_layer_size+1)\n",
    "initial_theta2 = randInitializeXavier(hidden_layer_size+2,num_labels)\n",
    "initial_theta = np.array([initial_theta1, initial_theta2], dtype=object)\n",
    "\n",
    "theta,J_history = gradientDescent(X,y,initial_theta,0.1,100,0.1,input_layer_size,hidden_layer_size,num_labels, reg=False)\n",
    "theta_reg, J_history_reg = gradientDescent(X,y,initial_theta,0.1,100,0.1,input_layer_size,hidden_layer_size,num_labels, reg=True)\n",
    "\n",
    "\n",
    "diff1, diff2 = check_grad(X,y,theta,input_layer_size,hidden_layer_size,num_labels, 0.1)[:2]\n",
    "diff1_reg, diff2_reg = check_grad(X,y,theta_reg,input_layer_size,hidden_layer_size,num_labels, 0.1)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34167554133918315\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(diff1)+np.mean(diff2)+np.mean(diff2)+np.mean(diff1_reg))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-86aa22cf04f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "np.concatenate(diff1.values,diff2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96943613\n",
      "Iteration 2, loss = 1.09748659\n",
      "Iteration 3, loss = 0.95305751\n",
      "Iteration 4, loss = 0.92041925\n",
      "Iteration 5, loss = 0.90890610\n",
      "Iteration 6, loss = 0.90144589\n",
      "Iteration 7, loss = 0.89908494\n",
      "Iteration 8, loss = 0.89452317\n",
      "Iteration 9, loss = 0.89231384\n",
      "Iteration 10, loss = 0.88967210\n",
      "Iteration 11, loss = 0.88696859\n",
      "Iteration 12, loss = 0.88640516\n",
      "Iteration 13, loss = 0.88540775\n",
      "Iteration 14, loss = 0.88360903\n",
      "Iteration 15, loss = 0.88558081\n",
      "Iteration 16, loss = 0.88310904\n",
      "Iteration 17, loss = 0.88422866\n",
      "Iteration 18, loss = 0.88183852\n",
      "Iteration 19, loss = 0.88049820\n",
      "Iteration 20, loss = 0.88177007\n",
      "Iteration 21, loss = 0.88082275\n",
      "Iteration 22, loss = 0.87866952\n",
      "Iteration 23, loss = 0.88061218\n",
      "Iteration 24, loss = 0.87883549\n",
      "Iteration 25, loss = 0.87754959\n",
      "Iteration 26, loss = 0.87928373\n",
      "Iteration 27, loss = 0.87859331\n",
      "Iteration 28, loss = 0.87966263\n",
      "Iteration 29, loss = 0.87958360\n",
      "Iteration 30, loss = 0.87793879\n",
      "Iteration 31, loss = 0.87772723\n",
      "Iteration 32, loss = 0.87869033\n",
      "Iteration 33, loss = 0.87550016\n",
      "Iteration 34, loss = 0.87639622\n",
      "Iteration 35, loss = 0.87669465\n",
      "Iteration 36, loss = 0.87635596\n",
      "Iteration 37, loss = 0.87836282\n",
      "Iteration 38, loss = 0.87551485\n",
      "Iteration 39, loss = 0.87660155\n",
      "Iteration 40, loss = 0.87521437\n",
      "Iteration 41, loss = 0.87440200\n",
      "Iteration 42, loss = 0.87687400\n",
      "Iteration 43, loss = 0.87766480\n",
      "Iteration 44, loss = 0.87458389\n",
      "Iteration 45, loss = 0.87413225\n",
      "Iteration 46, loss = 0.87516929\n",
      "Iteration 47, loss = 0.87437568\n",
      "Iteration 48, loss = 0.87383284\n",
      "Iteration 49, loss = 0.87415163\n",
      "Iteration 50, loss = 0.87348628\n",
      "Iteration 51, loss = 0.87267411\n",
      "Iteration 52, loss = 0.87343209\n",
      "Iteration 53, loss = 0.87308508\n",
      "Iteration 54, loss = 0.87298538\n",
      "Iteration 55, loss = 0.87383221\n",
      "Iteration 56, loss = 0.87463217\n",
      "Iteration 57, loss = 0.87196394\n",
      "Iteration 58, loss = 0.87306421\n",
      "Iteration 59, loss = 0.87251298\n",
      "Iteration 60, loss = 0.87391372\n",
      "Iteration 61, loss = 0.87408099\n",
      "Iteration 62, loss = 0.87416215\n",
      "Iteration 63, loss = 0.87303025\n",
      "Iteration 64, loss = 0.87379695\n",
      "Iteration 65, loss = 0.87290525\n",
      "Iteration 66, loss = 0.87131742\n",
      "Iteration 67, loss = 0.87315356\n",
      "Iteration 68, loss = 0.87364672\n",
      "Iteration 69, loss = 0.87135029\n",
      "Iteration 70, loss = 0.87315288\n",
      "Iteration 71, loss = 0.87213109\n",
      "Iteration 72, loss = 0.87209987\n",
      "Iteration 73, loss = 0.87295355\n",
      "Iteration 74, loss = 0.87346900\n",
      "Iteration 75, loss = 0.87201143\n",
      "Iteration 76, loss = 0.87173729\n",
      "Iteration 77, loss = 0.87236014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "12.50048828125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "start = time.time()\n",
    "clf = MLPClassifier(solver='sgd', alpha=1, hidden_layer_sizes=(25,), random_state=1, activation='logistic', learning_rate_init=0.3, max_iter=300, verbose=True)\n",
    "clf.fit(X, y.flatten())\n",
    "end = time.time()\n",
    "passed = end-start\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 91.28 %\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X)\n",
    "print(\"Training Set Accuracy:\",sum(pred[:,np.newaxis]==y)[0]/len(y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
