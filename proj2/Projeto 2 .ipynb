{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imageMNIST.csv\", header=None, low_memory=False)\n",
    "df_y = pd.read_csv(\"labelMNIST.csv\", header=None)\n",
    "\n",
    "def change_type(x):\n",
    "    if(isinstance(x, int)):\n",
    "        return float(x)\n",
    "    elif(isinstance(x, str)):\n",
    "        return float(x.replace(\",\", \".\"))\n",
    "\n",
    "df = df.applymap(change_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoide\n",
    "def sigmoid(z):\n",
    "    g = (1 + np.exp(-z))**(-1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid*(1-sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(X,y,theta,size_in,size_hidden,size_out, Lambda):\n",
    "    epsolon = 1e-4\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "#     theta_p1 = theta1+epsolon \n",
    "#     theta_m1 = theta1-epsolon\n",
    "    theta_p1 = np.array([theta1+epsolon, theta2], dtype=object)\n",
    "    theta_m1 = np.array([theta1-epsolon, theta2], dtype=object)\n",
    "    \n",
    "    theta_p2 = np.array([theta1, theta2+epsolon], dtype=object)\n",
    "    theta_m2 = np.array([theta1, theta2-epsolon], dtype=object)\n",
    "    \n",
    "    J_p1, grad1, grad2 = costFunction(X,y,theta_p1,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_p1, grad1_reg, grad2_reg = costFunction(X,y,theta_p1,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_m1, grad1, grad2 = costFunction(X,y,theta_m1,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_m1, grad1_reg, grad2_reg = costFunction(X,y,theta_m1,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_p2, grad1, grad2 = costFunction(X,y,theta_p2,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_p2, grad1_reg, grad2_reg = costFunction(X,y,theta_p2,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    J_m2, grad1, grad2 = costFunction(X,y,theta_m2,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J_m2, grad1_reg, grad2_reg = costFunction(X,y,theta_m2,size_in,size_hidden,size_out, Lambda=True)\n",
    "    \n",
    "    J, grad1, grad2 = costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=False)\n",
    "    reg_J, grad1_reg, grad2_reg = costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=True)\n",
    "    \n",
    "    check_delta1 = (J_p1-J_m1) / (2*epsolon)\n",
    "    check_delta2 = (J_p2-J_m2) / (2*epsolon)\n",
    "    \n",
    "    check_delta1_reg = (reg_J_p1-reg_J_m1) / (2*epsolon)\n",
    "    check_delta2_reg = (reg_J_p2-reg_J_m2) / (2*epsolon)\n",
    "    \n",
    "    diff1 = np.abs(check_delta1 - grad1)\n",
    "    diff2 = np.abs(check_delta2 - grad2)\n",
    "    diff1_reg = np.abs(check_delta1_reg - grad1_reg)\n",
    "    diff2_reg = np.abs(check_delta2_reg - grad2_reg)\n",
    "    \n",
    "    \n",
    "    return (diff1, diff2, diff1_reg, diff2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y_onehot = np.zeros((m,size_out))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    \n",
    "    y_onehot = np.zeros((y.size, 11))\n",
    "    y_onehot[np.arange(y.size),y.flatten()] = 1\n",
    "    y_onehot = y_onehot[:,1:]\n",
    "    \n",
    "    for j in range(size_out):\n",
    "        J = J + sum(-y_onehot[:,j]*np.log(a2[:,j])-(1-y_onehot[:,j])*np.log(1-a2[:,j]))\n",
    "        \n",
    "    J = 1/m*J\n",
    "    \n",
    "    if(reg):\n",
    "        J = J + Lambda/l(2*m)*(np.sum(theta1[:,1:]**2)+np.sum(theta2[:,1:]**2))\n",
    "                                 \n",
    "    grad1 = np.zeros((theta1.shape))\n",
    "    grad2 = np.zeros((theta2.shape))\n",
    "                                 \n",
    "    for i in range(m):\n",
    "        xi = X[i,:]\n",
    "        a1i = a1[i,:]\n",
    "        a2i = a2[i,:]\n",
    "        d2 = a2i - y_onehot[i,:]\n",
    "        d1 = theta2.T.dot(d2.T) * sigmoidGradient(np.hstack((1,xi.dot(theta1.T))))\n",
    "        grad1 = grad1 + d1[1:][:,np.newaxis].dot(xi[:,np.newaxis].T)\n",
    "        grad2 = grad2 + d2.T[:,np.newaxis].dot(a1i[:,np.newaxis].T)\n",
    "                                 \n",
    "    grad1 = grad1/m\n",
    "    grad2 = grad2/m\n",
    "    \n",
    "    if(reg):\n",
    "        grad1 = grad1 + (Lambda/m)*np.hstack((np.zeros((theta1.shape[0],1)),theta1[:,1:]))\n",
    "        grad2 = grad2 + (Lambda/m)*np.hstack((np.zeros((theta2.shape[0],1)),theta2[:,1:]))\n",
    "                                 \n",
    "    return J,grad1,grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##peguei do site amigo https://prateekvishnu.medium.com/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
    "def randInitializeXavier(size_in, size_out):\n",
    "    w=np.random.randn(size_out,size_in)*np.sqrt(1/size_in) \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(nbr_iter)):\n",
    "        theta = np.array([theta1, theta2], dtype=object)\n",
    "        if(reg):\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=True)\n",
    "        else:\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=False)\n",
    "        \n",
    "        theta1 = theta1 - (alpha*grad1)\n",
    "        theta2 = theta2 - (alpha*grad2)\n",
    "\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    nn_paramsFinal = np.array([theta1,theta2], dtype=object)\n",
    "    return nn_paramsFinal, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,theta1,theta2):\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    return np.argmax(a2,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train examples:\t3000\n",
      "# Validation examples:\t1000\n",
      "# Train examples:\t1000\n"
     ]
    }
   ],
   "source": [
    "X = df.values\n",
    "y = df_y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"# Train examples:\\t{}\".format(X_train.shape[0]))\n",
    "print(\"# Validation examples:\\t{}\".format(X_val.shape[0]))\n",
    "print(\"# Train examples:\\t{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532fc6f7b4d49588f84d806c223f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 50\n",
    "num_labels = 10\n",
    "\n",
    "initial_theta1 = randInitializeXavier(input_layer_size+1,hidden_layer_size+1)\n",
    "initial_theta2 = randInitializeXavier(hidden_layer_size+2,num_labels)\n",
    "initial_theta = np.array([initial_theta1, initial_theta2], dtype=object)\n",
    "\n",
    "# X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels):\n",
    "#0.001, 0.003, 0.01, 0.03, 0.1,\n",
    "alpha = 1\n",
    "theta,J_history_10 = gradientDescent(X_train,y_train,initial_theta,alpha,200,1,input_layer_size,hidden_layer_size,num_labels, reg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 92.0 %\n"
     ]
    }
   ],
   "source": [
    "theta1 = theta[0]\n",
    "theta2 = theta[1]\n",
    "pred = prediction(X_test,theta1,theta2)\n",
    "print(\"Training Set Accuracy:\", np.sum(pred==y_test.flatten())/len(y_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(J_history_10, label=\"lambda={}\".format(alpha))\n",
    "plt.legend()\n",
    "# plt.savefig(\"lambda{}.jpg\".format(str(alpha)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232af3a3d8454313a7bfb07d63458b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311ae95b9d8d4a9f811409e164ba8030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small network\n",
    "samples = 50\n",
    "rand_index = random.sample([i for i in range(df.shape[0])], k=samples)\n",
    "\n",
    "X = df.iloc[rand_index].values\n",
    "y = df_y.iloc[rand_index].values\n",
    "\n",
    "\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 5\n",
    "num_labels = 10\n",
    "\n",
    "initial_theta1 = randInitializeXavier(input_layer_size+1,hidden_layer_size+1)\n",
    "initial_theta2 = randInitializeXavier(hidden_layer_size+2,num_labels)\n",
    "initial_theta = np.array([initial_theta1, initial_theta2], dtype=object)\n",
    "\n",
    "theta,J_history = gradientDescent(X,y,initial_theta,0.1,100,0.1,input_layer_size,hidden_layer_size,num_labels, reg=False)\n",
    "theta_reg, J_history_reg = gradientDescent(X,y,initial_theta,0.1,100,0.1,input_layer_size,hidden_layer_size,num_labels, reg=True)\n",
    "\n",
    "\n",
    "diff1, diff2 = check_grad(X,y,theta,input_layer_size,hidden_layer_size,num_labels, 0.1)[:2]\n",
    "diff1_reg, diff2_reg = check_grad(X,y,theta_reg,input_layer_size,hidden_layer_size,num_labels, 0.1)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34167554133918315\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(diff1)+np.mean(diff2)+np.mean(diff2)+np.mean(diff1_reg))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-86aa22cf04f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "np.concatenate(diff1.values,diff2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96943613\n",
      "Iteration 2, loss = 1.09748659\n",
      "Iteration 3, loss = 0.95305751\n",
      "Iteration 4, loss = 0.92041925\n",
      "Iteration 5, loss = 0.90890610\n",
      "Iteration 6, loss = 0.90144589\n",
      "Iteration 7, loss = 0.89908494\n",
      "Iteration 8, loss = 0.89452317\n",
      "Iteration 9, loss = 0.89231384\n",
      "Iteration 10, loss = 0.88967210\n",
      "Iteration 11, loss = 0.88696859\n",
      "Iteration 12, loss = 0.88640516\n",
      "Iteration 13, loss = 0.88540775\n",
      "Iteration 14, loss = 0.88360903\n",
      "Iteration 15, loss = 0.88558081\n",
      "Iteration 16, loss = 0.88310904\n",
      "Iteration 17, loss = 0.88422866\n",
      "Iteration 18, loss = 0.88183852\n",
      "Iteration 19, loss = 0.88049820\n",
      "Iteration 20, loss = 0.88177007\n",
      "Iteration 21, loss = 0.88082275\n",
      "Iteration 22, loss = 0.87866952\n",
      "Iteration 23, loss = 0.88061218\n",
      "Iteration 24, loss = 0.87883549\n",
      "Iteration 25, loss = 0.87754959\n",
      "Iteration 26, loss = 0.87928373\n",
      "Iteration 27, loss = 0.87859331\n",
      "Iteration 28, loss = 0.87966263\n",
      "Iteration 29, loss = 0.87958360\n",
      "Iteration 30, loss = 0.87793879\n",
      "Iteration 31, loss = 0.87772723\n",
      "Iteration 32, loss = 0.87869033\n",
      "Iteration 33, loss = 0.87550016\n",
      "Iteration 34, loss = 0.87639622\n",
      "Iteration 35, loss = 0.87669465\n",
      "Iteration 36, loss = 0.87635596\n",
      "Iteration 37, loss = 0.87836282\n",
      "Iteration 38, loss = 0.87551485\n",
      "Iteration 39, loss = 0.87660155\n",
      "Iteration 40, loss = 0.87521437\n",
      "Iteration 41, loss = 0.87440200\n",
      "Iteration 42, loss = 0.87687400\n",
      "Iteration 43, loss = 0.87766480\n",
      "Iteration 44, loss = 0.87458389\n",
      "Iteration 45, loss = 0.87413225\n",
      "Iteration 46, loss = 0.87516929\n",
      "Iteration 47, loss = 0.87437568\n",
      "Iteration 48, loss = 0.87383284\n",
      "Iteration 49, loss = 0.87415163\n",
      "Iteration 50, loss = 0.87348628\n",
      "Iteration 51, loss = 0.87267411\n",
      "Iteration 52, loss = 0.87343209\n",
      "Iteration 53, loss = 0.87308508\n",
      "Iteration 54, loss = 0.87298538\n",
      "Iteration 55, loss = 0.87383221\n",
      "Iteration 56, loss = 0.87463217\n",
      "Iteration 57, loss = 0.87196394\n",
      "Iteration 58, loss = 0.87306421\n",
      "Iteration 59, loss = 0.87251298\n",
      "Iteration 60, loss = 0.87391372\n",
      "Iteration 61, loss = 0.87408099\n",
      "Iteration 62, loss = 0.87416215\n",
      "Iteration 63, loss = 0.87303025\n",
      "Iteration 64, loss = 0.87379695\n",
      "Iteration 65, loss = 0.87290525\n",
      "Iteration 66, loss = 0.87131742\n",
      "Iteration 67, loss = 0.87315356\n",
      "Iteration 68, loss = 0.87364672\n",
      "Iteration 69, loss = 0.87135029\n",
      "Iteration 70, loss = 0.87315288\n",
      "Iteration 71, loss = 0.87213109\n",
      "Iteration 72, loss = 0.87209987\n",
      "Iteration 73, loss = 0.87295355\n",
      "Iteration 74, loss = 0.87346900\n",
      "Iteration 75, loss = 0.87201143\n",
      "Iteration 76, loss = 0.87173729\n",
      "Iteration 77, loss = 0.87236014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "12.50048828125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "start = time.time()\n",
    "clf = MLPClassifier(solver='sgd', alpha=1, hidden_layer_sizes=(25,), random_state=1, activation='logistic', learning_rate_init=0.3, max_iter=300, verbose=True)\n",
    "clf.fit(X, y.flatten())\n",
    "end = time.time()\n",
    "passed = end-start\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 91.28 %\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X)\n",
    "print(\"Training Set Accuracy:\",sum(pred[:,np.newaxis]==y)[0]/len(y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
