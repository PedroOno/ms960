{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imageMNIST.csv\", header=None, low_memory=False)\n",
    "df_y = pd.read_csv(\"labelMNIST.csv\", header=None)\n",
    "\n",
    "def change_type(x):\n",
    "    if(isinstance(x, int)):\n",
    "        return float(x)\n",
    "    elif(isinstance(x, str)):\n",
    "        return float(x.replace(\",\", \".\"))\n",
    "\n",
    "df = df.applymap(change_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoide\n",
    "def sigmoid(z):\n",
    "    g = (1 + np.exp(-z))**(-1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid*(1-sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(X,y,theta,size_in,size_hidden,size_out, Lambda):\n",
    "    epsolon = 1e-4\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "#     theta_p1 = theta1+epsolon \n",
    "#     theta_m1 = theta1-epsolon\n",
    "    theta_p1 = np.vstack(theta1+epsolon, theta2)\n",
    "    theta_m1 = np.vstack(theta1-epsolon, theta2)\n",
    "    \n",
    "    theta_p2 = np.vstack(theta1, theta2+epsolon)\n",
    "    theta_m2 = np.vstack(theta1, theta2-epsolon)\n",
    "    \n",
    "    J_p1, grad1, grad2, reg_J_p1, grad1_reg, grad2_reg = costFunction(X,y,theta_p1,size_in,size_hidden,size_out, Lambda)\n",
    "    J_m1, grad1, grad2, reg_J_m1, grad1_reg, grad2_reg = costFunction(X,y,theta_m1,size_in,size_hidden,size_out, Lambda)\n",
    "    \n",
    "    J_p2, grad1, grad2, reg_J_p2, grad1_reg, grad2_reg = costFunction(X,y,theta_p2,size_in,size_hidden,size_out, Lambda)\n",
    "    J_m2, grad1, grad2, reg_J_m2, grad1_reg, grad2_reg = costFunction(X,y,theta_m2,size_in,size_hidden,size_out, Lambda)\n",
    "    \n",
    "    J, grad1, grad2, reg_J, grad1_reg, grad2_reg = costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda)\n",
    "    \n",
    "    check_delta1 = (J_p1-J_m1) / (2*epsolon)\n",
    "    check_delta2 = (J_p2-J_m2) / (2*epsolon)\n",
    "    \n",
    "    check_delta1_reg = (reg_J_p1-reg_J_m1) / (2*epsolon)\n",
    "    check_delta2_reg = (reg_J_p2-reg_J_m2) / (2*epsolon)\n",
    "    \n",
    "    diff1 = np.abs(check_delta1 - grad1)\n",
    "    diff2 = np.abs(check_delta2 - grad2)\n",
    "    diff1_reg = np.abs(check_delta1_reg - grad1_reg)\n",
    "    diff2_reg = np.abs(check_delta2_reg - grad2_reg)\n",
    "    \n",
    "    return (diff1, diff2, diff1_reg, diff2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X,y,theta,size_in,size_hidden,size_out, Lambda, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y_onehot = np.zeros((m,size_out))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    \n",
    "    y_onehot = np.zeros((y.size, y.max()+1))\n",
    "    y_onehot[np.arange(y.size),y.flatten()] = 1\n",
    "    y_onehot = y_onehot[:,1:]\n",
    "    \n",
    "    for j in range(size_out):\n",
    "        J = J + sum(-y_onehot[:,j]*np.log(a2[:,j])-(1-y_onehot[:,j])*np.log(1-a2[:,j]))\n",
    "        \n",
    "    J = 1/m*J\n",
    "    \n",
    "    if(reg):\n",
    "        J = J + Lambda/(2*m)*(np.sum(theta1[:,1:]**2)+np.sum(theta2[:,1:]**2))\n",
    "                                 \n",
    "    grad1 = np.zeros((theta1.shape))\n",
    "    grad2 = np.zeros((theta2.shape))\n",
    "                                 \n",
    "    for i in range(m):\n",
    "        xi = X[i,:]\n",
    "        a1i = a1[i,:]\n",
    "        a2i = a2[i,:]\n",
    "        d2 = a2i - y_onehot[i,:]\n",
    "        d1 = theta2.T.dot(d2.T) * sigmoidGradient(np.hstack((1,xi.dot(theta1.T))))\n",
    "        grad1 = grad1 + d1[1:][:,np.newaxis].dot(xi[:,np.newaxis].T)\n",
    "        grad2 = grad2 + d2.T[:,np.newaxis].dot(a1i[:,np.newaxis].T)\n",
    "                                 \n",
    "    grad1 = grad1/m\n",
    "    grad2 = grad2/m\n",
    "    \n",
    "    if(reg):\n",
    "        grad1 = grad1 + (Lambda/m)*np.hstack((np.zeros((theta1.shape[0],1)),theta1[:,1:]))\n",
    "        grad2 = grad2 + (Lambda/m)*np.hstack((np.zeros((theta2.shape[0],1)),theta2[:,1:]))\n",
    "                                 \n",
    "    return J,grad1,grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##peguei do site amigo https://prateekvishnu.medium.com/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
    "def randInitializeXavier(size_in, size_out):\n",
    "    w=np.random.randn(size_out,size_in)*np.sqrt(1/size_in) \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels, reg=False):\n",
    "    theta1 = theta[0]\n",
    "    theta2 = theta[1]\n",
    "\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(nbr_iter)):\n",
    "        theta = np.array([theta1, theta2], dtype=object)\n",
    "        if(reg):\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=True)\n",
    "        else:\n",
    "            cost,grad1,grad2, = costFunction(X,y,theta,input_layer_size,hidden_layer_size,num_labels,Lambda, reg=True)\n",
    "        \n",
    "        theta1 = theta1 - (alpha*grad1)\n",
    "        theta2 = theta2 - (alpha*grad2)\n",
    "\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    nn_paramsFinal = np.array([theta1,theta2], dtype=object)\n",
    "    return nn_paramsFinal, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,theta1,theta2):\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    \n",
    "    a1 = sigmoid(X.dot(theta1.T))\n",
    "    a1 = np.hstack((np.ones((m,1)),a1))\n",
    "    a2 = sigmoid(a1.dot(theta2.T))\n",
    "    \n",
    "    return np.argmax(a2,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train examples:\t3000\n",
      "# Validation examples:\t1000\n",
      "# Train examples:\t1000\n"
     ]
    }
   ],
   "source": [
    "X = df.values\n",
    "y = df_y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"# Train examples:\\t{}\".format(X_train.shape[0]))\n",
    "print(\"# Validation examples:\\t{}\".format(X_val.shape[0]))\n",
    "print(\"# Train examples:\\t{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5245cf49313c46cbaa1b6b8edfa1cfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "initial_theta1 = randInitializeXavier(input_layer_size+1,hidden_layer_size+1)\n",
    "initial_theta2 = randInitializeXavier(hidden_layer_size+2,num_labels)\n",
    "initial_theta = np.array([initial_theta1, initial_theta2], dtype=object)\n",
    "\n",
    "# X,y,theta,alpha,nbr_iter,Lambda,input_layer_size,hidden_layer_size,num_labels):\n",
    "\n",
    "theta,J_history = gradientDescent(X,y,initial_theta,0.1,200,0.1,input_layer_size,hidden_layer_size,num_labels, reg=True)\n",
    "theta1 = theta[0]\n",
    "theta2 = theta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 73.16 %\n"
     ]
    }
   ],
   "source": [
    "pred = prediction(X,theta1,theta2)\n",
    "print(\"Training Set Accuracy:\", np.sum(pred==y.flatten())/len(y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(J_history, label=\"cost\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999999, 0.99999999, 0.99999998, 1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [1,2,3,4]\n",
    "res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rosen() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8763af1668e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrosen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: rosen() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "rosen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96943613\n",
      "Iteration 2, loss = 1.09748659\n",
      "Iteration 3, loss = 0.95305751\n",
      "Iteration 4, loss = 0.92041925\n",
      "Iteration 5, loss = 0.90890610\n",
      "Iteration 6, loss = 0.90144589\n",
      "Iteration 7, loss = 0.89908494\n",
      "Iteration 8, loss = 0.89452317\n",
      "Iteration 9, loss = 0.89231384\n",
      "Iteration 10, loss = 0.88967210\n",
      "Iteration 11, loss = 0.88696859\n",
      "Iteration 12, loss = 0.88640516\n",
      "Iteration 13, loss = 0.88540775\n",
      "Iteration 14, loss = 0.88360903\n",
      "Iteration 15, loss = 0.88558081\n",
      "Iteration 16, loss = 0.88310904\n",
      "Iteration 17, loss = 0.88422866\n",
      "Iteration 18, loss = 0.88183852\n",
      "Iteration 19, loss = 0.88049820\n",
      "Iteration 20, loss = 0.88177007\n",
      "Iteration 21, loss = 0.88082275\n",
      "Iteration 22, loss = 0.87866952\n",
      "Iteration 23, loss = 0.88061218\n",
      "Iteration 24, loss = 0.87883549\n",
      "Iteration 25, loss = 0.87754959\n",
      "Iteration 26, loss = 0.87928373\n",
      "Iteration 27, loss = 0.87859331\n",
      "Iteration 28, loss = 0.87966263\n",
      "Iteration 29, loss = 0.87958360\n",
      "Iteration 30, loss = 0.87793879\n",
      "Iteration 31, loss = 0.87772723\n",
      "Iteration 32, loss = 0.87869033\n",
      "Iteration 33, loss = 0.87550016\n",
      "Iteration 34, loss = 0.87639622\n",
      "Iteration 35, loss = 0.87669465\n",
      "Iteration 36, loss = 0.87635596\n",
      "Iteration 37, loss = 0.87836282\n",
      "Iteration 38, loss = 0.87551485\n",
      "Iteration 39, loss = 0.87660155\n",
      "Iteration 40, loss = 0.87521437\n",
      "Iteration 41, loss = 0.87440200\n",
      "Iteration 42, loss = 0.87687400\n",
      "Iteration 43, loss = 0.87766480\n",
      "Iteration 44, loss = 0.87458389\n",
      "Iteration 45, loss = 0.87413225\n",
      "Iteration 46, loss = 0.87516929\n",
      "Iteration 47, loss = 0.87437568\n",
      "Iteration 48, loss = 0.87383284\n",
      "Iteration 49, loss = 0.87415163\n",
      "Iteration 50, loss = 0.87348628\n",
      "Iteration 51, loss = 0.87267411\n",
      "Iteration 52, loss = 0.87343209\n",
      "Iteration 53, loss = 0.87308508\n",
      "Iteration 54, loss = 0.87298538\n",
      "Iteration 55, loss = 0.87383221\n",
      "Iteration 56, loss = 0.87463217\n",
      "Iteration 57, loss = 0.87196394\n",
      "Iteration 58, loss = 0.87306421\n",
      "Iteration 59, loss = 0.87251298\n",
      "Iteration 60, loss = 0.87391372\n",
      "Iteration 61, loss = 0.87408099\n",
      "Iteration 62, loss = 0.87416215\n",
      "Iteration 63, loss = 0.87303025\n",
      "Iteration 64, loss = 0.87379695\n",
      "Iteration 65, loss = 0.87290525\n",
      "Iteration 66, loss = 0.87131742\n",
      "Iteration 67, loss = 0.87315356\n",
      "Iteration 68, loss = 0.87364672\n",
      "Iteration 69, loss = 0.87135029\n",
      "Iteration 70, loss = 0.87315288\n",
      "Iteration 71, loss = 0.87213109\n",
      "Iteration 72, loss = 0.87209987\n",
      "Iteration 73, loss = 0.87295355\n",
      "Iteration 74, loss = 0.87346900\n",
      "Iteration 75, loss = 0.87201143\n",
      "Iteration 76, loss = 0.87173729\n",
      "Iteration 77, loss = 0.87236014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "12.50048828125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "start = time.time()\n",
    "clf = MLPClassifier(solver='sgd', alpha=1, hidden_layer_sizes=(25,), random_state=1, activation='logistic', learning_rate_init=0.3, max_iter=300, verbose=True)\n",
    "clf.fit(X, y.flatten())\n",
    "end = time.time()\n",
    "passed = end-start\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 91.28 %\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X)\n",
    "print(\"Training Set Accuracy:\",sum(pred[:,np.newaxis]==y)[0]/len(y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
